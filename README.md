# ğŸš€ Glamira Product Data Scraper

A production-ready web scraper for extracting product data from Glamira jewelry websites across multiple domains.

## âœ… **Project Status**
- **âœ… Fully Tested**: 100% success rate on test batch
- **âœ… Production Ready**: Handles 553,000 domain-product combinations
- **âœ… Field Filtering**: Extracts 28 specific product fields
- **âœ… Robust Error Handling**: 5-attempt retry logic for 403 errors
- **âœ… Concurrent Processing**: 3 worker threads with rate limiting

## ğŸ—ï¸ **Clean Project Structure**

```
ğŸ“ glamira_prdinfo_crawling/
â”œâ”€â”€ ğŸ main.py                     # Main scraper engine
â”œâ”€â”€ ğŸš€ launcher.py                 # Quick command launcher
â”œâ”€â”€ ğŸ“‹ requirements.txt            # Dependencies
â”œâ”€â”€ ğŸ—„ï¸ checkpoint.db              # Progress database
â”œâ”€â”€ ğŸ“„ PROJECT_OVERVIEW.md        # Detailed overview
â”œâ”€â”€ ğŸ”’ .gitignore                 # Git ignore rules
â”‚
â”œâ”€â”€ ğŸ“‚ data/                       # All data files
â”‚   â”œâ”€â”€ ğŸ“‚ input_data/            # Source CSV files
â”‚   â””â”€â”€ ğŸ“‚ scraped/               # Output results
â”‚
â”œâ”€â”€ ğŸ“‚ scripts/                   # Utility scripts
â”‚   â”œâ”€â”€ ğŸ” monitor.py             # Progress monitoring
â”‚   â”œâ”€â”€ ğŸ­ run_full_scraping.py   # Production scraper
â”‚   â””â”€â”€ ğŸ§ª start_scraping.py      # Batch testing
â”‚
â”œâ”€â”€ ğŸ“‚ tests/                     # Test suite
â”‚   â”œâ”€â”€ ğŸ§ª test_filtering.py      # Field filtering tests
â”‚   â”œâ”€â”€ âš¡ quick_test.py          # Quick validation
â”‚   â””â”€â”€ ğŸ“Š test_*.py              # Other test scripts
â”‚
â”œâ”€â”€ ğŸ“‚ logs/                      # Log files
â”‚   â””â”€â”€ ğŸ“‹ *.log                  # Scraping logs
â”‚
â””â”€â”€ ğŸ“‚ docs/                      # Documentation
    â”œâ”€â”€ ğŸ“– README.md              # Full documentation
    â””â”€â”€ ğŸ“‹ FIELD_FILTERING_UPDATE.md
```

## ğŸš€ **Quick Start**

### Option 1: Use the Launcher (Recommended)
```bash
python launcher.py
```

### Option 2: Direct Commands
```bash
# Batch testing (recommended first)
python scripts/start_scraping.py

# Monitor progress
python scripts/monitor.py

# Manage checkpoint database
python scripts/checkpoint_manager.py

# Full production scraping
python scripts/run_full_scraping.py
```

## ğŸ“Š **Data Scope**
- **Domains**: 79 Glamira websites (glamira.com, glamira.at, etc.)
- **Products**: 7,000 unique product IDs
- **Total Combinations**: 553,000 URLs to scrape
- **Success Rate**: 100% (based on testing)

## ğŸ¯ **Extracted Fields (28 total)**
```
product_id, name, sku, attribute_set_id, attribute_set, type_id, 
price, min_price, max_price, min_price_format, max_price_format, 
gold_weight, none_metal_weight, fixed_silver_weight, material_design, 
qty, collection, collection_id, product_type, product_type_value, 
category, category_name, store_code, platinum_palladium_info_in_alloy, 
bracelet_without_chain, show_popup_quantity_eternity, visible_contents, gender
```

## âš™ï¸ **Configuration**
- **Python Environment**: `C:\Users\cuongdq\AppData\Local\miniconda3\envs\venv\python.exe`
- **Concurrent Workers**: 3 threads
- **Retry Logic**: 5 attempts for 403 errors
- **Page Load Delay**: 3 seconds
- **Output Format**: Filtered JSON files

## ğŸ“ˆ **Features**
- âœ… **Concurrent Processing**: 3 worker threads for efficiency
- âœ… **Smart Retry Logic**: Handles 403 errors with exponential backoff
- âœ… **Progress Tracking**: SQLite database checkpointing
- âœ… **Auto-Clear Database**: Automatically clears checkpoint DB when completed
- âœ… **Field Filtering**: Extracts only required data fields
- âœ… **Comprehensive Logging**: Detailed logs for monitoring
- âœ… **Resume Capability**: Can resume interrupted scraping
- âœ… **Database Management**: Built-in checkpoint database tools

## ğŸ”§ **Installation**
```bash
# Install dependencies
pip install -r requirements.txt

# Verify setup
python tests/quick_test.py
```

## ğŸ“‹ **Usage Examples**

### Test Before Full Run
```bash
# Test with small batch first
python scripts/start_scraping.py

# Check results
python scripts/monitor.py
```

### Production Scraping
```bash
# Run full scraping (553k combinations)
python scripts/run_full_scraping.py

# Monitor in another terminal
python scripts/monitor.py
```

### Check Results
```bash
# View scraped files
ls data/scraped/output/

# Test field filtering on existing data
python tests/test_filtering.py
```

## ğŸ“Š **Output**
- **Location**: `data/scraped/output/`
- **Format**: `{domain}_{product_id}.json`
- **Size**: ~28 fields per product (filtered)
- **Progress**: Tracked in `checkpoint.db`

## ğŸš¨ **Important Notes**
1. **Rate Limiting**: Built-in 3-second delays between requests
2. **Error Handling**: Automatic retries for temporary failures
3. **Progress Saving**: Can safely interrupt and resume
4. **Resource Usage**: Optimized for 3 concurrent workers

## ğŸ“ **Support**
- Check `logs/` folder for detailed error information
- Use `python scripts/monitor.py` for real-time progress
- Review `docs/README.md` for comprehensive documentation

---
**Status**: Production Ready ğŸš€ | **Last Updated**: September 5, 2025
